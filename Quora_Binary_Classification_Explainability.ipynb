{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yhTxD60crmq"
      },
      "source": [
        "Install LIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_etbfiSzg6Gy"
      },
      "outputs": [],
      "source": [
        "pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3pERlamcuMI"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMS-tbB7g106"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iLfJ_Tocv69"
      },
      "source": [
        "Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3FjV8Tlg107"
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "train_df = pd.read_csv(\"/content/Data/train.csv\")\n",
        "print(\"Train shape : \", train_df.shape)\n",
        "print(train_df.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH0Nsc6kgmWY"
      },
      "source": [
        "Display rows with NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cewnXmF0QnuA"
      },
      "outputs": [],
      "source": [
        "# Display rows with NaN values\n",
        "nan_rows = train_df[train_df.isna().any(axis=1)]\n",
        "print(nan_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q44voEFvgsCl"
      },
      "source": [
        "Print train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIwW3OGpQxkZ"
      },
      "outputs": [],
      "source": [
        "# Remove rows with NaN values\n",
        "train_df = train_df.dropna()\n",
        "print(\"Train shape : \", train_df.shape)\n",
        "print(train_df.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUdQBSkNgxKr"
      },
      "source": [
        "Train and val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6tS4MkPO-Gy"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDMkYnmSmQWx"
      },
      "outputs": [],
      "source": [
        "print(val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uAPZuNGg2qs"
      },
      "source": [
        "Select sample val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gOyQQWFO-Gy"
      },
      "outputs": [],
      "source": [
        "# Select specific rows from validation set based on qid for inspection\n",
        "df_select = pd.concat([val_df[val_df['qid'] == 'd61b098340966d9d6501'], val_df[val_df['qid'] == 'feb0053f32eda8483c9f']], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RexVFtabg7Qg"
      },
      "source": [
        "Select question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN9SnlxWO-Gz"
      },
      "outputs": [],
      "source": [
        "# Display the 'question_text' column of selected rows\n",
        "df_select.question_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8NwTVLhDPp"
      },
      "source": [
        "Print val data head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15qfs5eRO-Gz"
      },
      "outputs": [],
      "source": [
        "# Reset index of validation dataframe\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "print(val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIaP59ofhGm2"
      },
      "source": [
        "TF-IDF vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDJWt2d_O-Gz"
      },
      "outputs": [],
      "source": [
        "# Create a TF-IDF vectorizer and transform the training and validation data\n",
        "\n",
        "## vectorize to tf-idf vectors\n",
        "tfidf_vc = TfidfVectorizer(min_df = 10, max_features = 100000, analyzer = \"word\", ngram_range = (1, 2), stop_words = 'english', lowercase = True)\n",
        "train_vc = tfidf_vc.fit_transform(train_df[\"question_text\"])\n",
        "val_vc = tfidf_vc.transform(val_df[\"question_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJVYD-KAhIvQ"
      },
      "source": [
        "Logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BGMgTcVO-Gz"
      },
      "outputs": [],
      "source": [
        "# Train a Logistic Regression model on the training data\n",
        "\n",
        "model = LogisticRegression(C = 0.5, solver = \"sag\")\n",
        "model = model.fit(train_vc, train_df.target)\n",
        "\n",
        "# Predict on the validation data\n",
        "val_pred = model.predict(val_vc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUvupHKfhzhG"
      },
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjNvZTg9Sxdd"
      },
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "accuracy = accuracy_score(val_df.target, val_pred)\n",
        "precision = precision_score(val_df.target, val_pred)\n",
        "recall = recall_score(val_df.target, val_pred)\n",
        "f1 = f1_score(val_df.target, val_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbqjtuMfS6yQ"
      },
      "outputs": [],
      "source": [
        "# Print evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOTSNAfOS-NI"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix\n",
        "conf_matrix = confusion_matrix(val_df.target, val_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ky-C1p_vTAQc"
      },
      "outputs": [],
      "source": [
        "# Display classification report\n",
        "# Define class names\n",
        "class_names = [\"sincere\", \"insincere\"]\n",
        "class_report = classification_report(val_df.target, val_pred, target_names=class_names)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgcvMupFUye0"
      },
      "outputs": [],
      "source": [
        "# Filter the rows where target is 1\n",
        "target_1_rows = val_df[val_df['target'] == 1]\n",
        "\n",
        "# Print the filtered rows and their row indices\n",
        "print(\"Rows with target = 1:\")\n",
        "print(target_1_rows)\n",
        "\n",
        "print(\"\\nRow indices of rows with target = 1:\")\n",
        "print(target_1_rows.index.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv_CZKmuO-Gz"
      },
      "outputs": [],
      "source": [
        "# Select a specific instance from the validation set for explanation\n",
        "import numpy as np\n",
        "prediction_index = 138\n",
        "idx = int(val_df.index[prediction_index])\n",
        "# print(idx)\n",
        "c = make_pipeline(tfidf_vc, model)\n",
        "class_names = [\"sincere\", \"insincere\"]\n",
        "\n",
        "# Create a LIME text explainer\n",
        "explainer = LimeTextExplainer(class_names = class_names)\n",
        "\n",
        "# Explain the prediction for the selected instance\n",
        "exp = explainer.explain_instance(val_df[\"question_text\"][idx], c.predict_proba, num_features = 10)\n",
        "\n",
        "# Print the selected question text and its prediction probabilities\n",
        "print(val_df[\"question_text\"][idx])\n",
        "print(\"Probability (Insincere) =\", c.predict_proba([val_df[\"question_text\"][idx]])[0, 1])\n",
        "print(\"Probability (Sincere) =\", c.predict_proba([val_df[\"question_text\"][idx]])[0, 0])\n",
        "print(\"True Class is:\", class_names[int(val_df[\"target\"][idx])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apbnR46hO-G0"
      },
      "outputs": [],
      "source": [
        "# Get explanation weights as a list of tuples\n",
        "exp.as_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4uzRSDO-G0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcTjWm0tO-G0"
      },
      "outputs": [],
      "source": [
        "# Print original prediction probability\n",
        "print('Original prediction:',  model.predict_proba(val_vc[prediction_index])[0, 1])\n",
        "\n",
        "# Create a copy of the selected instance's TF-IDF vector and modify specific features\n",
        "tmp = val_vc[prediction_index].copy()\n",
        "tmp[0, tfidf_vc.vocabulary_['indians']] = 0\n",
        "tmp[0, tfidf_vc.vocabulary_['europeans']] = 0\n",
        "\n",
        "# Print prediction after removing specific features\n",
        "print('Prediction after removing some features:', model.predict_proba(tmp)[0, 1])\n",
        "\n",
        "# Print the difference in prediction probabilities\n",
        "print('Difference:', model.predict_proba(tmp)[0, 1] - model.predict_proba(val_vc[prediction_index])[0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xZc7vJUQO-G0"
      },
      "outputs": [],
      "source": [
        "# Display LIME explanation in a notebook\n",
        "exp.show_in_notebook(text=val_df[\"question_text\"][idx], labels=(1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOrOwD9jO-G0"
      },
      "outputs": [],
      "source": [
        "# Extract and plot LIME weights\n",
        "weights = OrderedDict(exp.as_list())\n",
        "lime_weights = pd.DataFrame({\"words\": list(weights.keys()), \"weights\": list(weights.values())})\n",
        "\n",
        "# Plot the feature weights\n",
        "sns.barplot(x = \"words\", y = \"weights\", data = lime_weights, palette=\"viridis\")\n",
        "plt.xticks(rotation = 45)\n",
        "plt.title(\"Sample {} features weights given by LIME\".format(idx))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}